---
title: "Practical Machine Learning Project - Wearable Computing"
author: "Fabio Barros"
date: "25 de março de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction  

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

### 1.1 R Packages 

In this project, we will use *caret* and *randomForest* packages.
```{r, message=FALSE}
# Loads R packages
library(caret)
library(randomForest)
```

##  2. Data Collection

First of all, we will download the data files, store them in the "data" folder and load datasets into the *training* and *testing* variables. 
```{r}
# Sets variables
trainingURL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainingFile <- "./data/pml-training.csv"
testingFile <- "./data/pml-testing.csv"

# Downloads raw data files
# if (!file.exists("./data")) {
#   dir.create("./data")
# 
# if (!file.exists(training.file)) {
#   download.file(trainingURL, destfile = trainingFile, method="curl")
# }
# if (!file.exists(testing.file)) {
#   download.file(testingURL, destfile = testingFile, method="curl")
# }

# Reads rawdata files
training <- read.csv(file = trainingFile, na.strings=c("NA","#DIV/0!",""), 
                         stringsAsFactors =  FALSE)
testing <- read.csv(file = testingFile, na.strings=c("NA","#DIV/0!",""), 
                        stringsAsFactors =  FALSE)

# Get datasets dimensions
dim(training)
dim(testing)
```

The *training* dataset contains `r dim(training)[2]` variables and `r dim(training)[1]` observations, while the *testing* dataset contains `r dim(testing)[2]` variables too, but less observations: `r dim(testing)[2]`.

The variable *classe* is the outcome to be predicted by the model. It has five possible values:

```{r}
unique(training$classe)
```

## 3. Data Preparation and Exploration 

Now, it's time to prepare and explore the collected data, cleaning and preprocessing it in order to be ready for modeling.

### 3.1 Cleaning Data

**a) Training dataset**
```{r}
# Removes variables which don't correspond to accelerometer measurements.
unusedCol <- grepl("^X|user|timestamp|window", names(training))
training <- training[, !unusedCol]

# Removes variables containing all NA missing values.
training <- training[, colSums(is.na(training)) == 0]

# Saves values of classe variable
classe <- training$classe

# Removes non-quantitative variables (also removes classe variable in training data)
training <- training[, (sapply(training, is.integer) | sapply(training, is.numeric))] 

# Converts integer variables to numeric
training <- as.data.frame(apply(training, 2, as.numeric))

# Restores values of classe variable to training data set.
training$classe <- factor(classe)

# Gets the dimension of dataset
dim(training)
```
Now, the number of variables of *training* dataset was reduced to: `r dim(training)[2]`.

**b) Testing dataset**

Likewise, the set of variables of *testing* dataset will be reduced. It will remain only the same variables contained in *training* dataset, except for the classe variable.

```{r}
# Removes ununsed variables of testing dataset
testing <- testing[, names(testing) %in% names(training)]

# Gets the dimension of dataset
dim(testing)
```
The resultant number of variables of *testing* dataset was reduced to: `r dim(training)[2]`. 


### 3.2 Preprocessing data

In this step, we will slice the *training* data, creating a new dataset called *validating*. We also preprocess all the datasets, normalizing them. 

```{r}
# Sets seed
set.seed(13173)

# Slices the training data
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)

# Creates a cross-validation set
validating <- training[-inTrain, ] 
dim(validating)

# Recreates the training set, removing observations of validating set.
training <- training[inTrain, ]
dim(training)

# Normalizes all the data
preProc <- preProcess(training[, -53], method = c("center", "scale")) # the 53th column is the outcome
training <- predict(preProc, training)
validating <- predict(preProc, validating)
testing <- predict(preProc, testing)

# Shows datasets heads
head(training[, c(1:5, 53)])
head(validating[, c(1:5, 53)])
head(testing[, 1:5])
```

Now, the datasets are ready for the next step: data modeling.

## 4. Data Modeling


```{r}
modelRF <- randomForest(classe ~ ., data = training)
predictions <- predict(modelRF, validating, type = "class")
confusionMatrix(predictions, validating$classe)
```

## Conclusions
